{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b37ceb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5cb41dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader('2201.11903.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3eb28b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "print(len(reader.pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4d0a110",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = reader.pages[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "059f7b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_text(reader):\n",
    "    full_text=\"\"\n",
    "    for page in reader.pages:\n",
    "        text = page.extract_text()\n",
    "        full_text += \"\\n\" + text\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6b89cd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = get_full_text(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c2121a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "openai = OpenAI(model_name=\"text-davinci-003\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "01a42900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_section_title(text):\n",
    "    text = text.split(\"\\n\")\n",
    "    title_sec = {}\n",
    "    sec_so_far = \"\"\n",
    "    prev_title = \"\"\n",
    "    title = \"\"\n",
    "    for para in text:\n",
    "        if re.search(\"^\\d+ [A-Za-z  -]+$\", para):\n",
    "            prev_title = title\n",
    "            title = para\n",
    "            title_sec[prev_title] = sec_so_far.strip()\n",
    "            sec_so_far=\"\"\n",
    "        else:\n",
    "            sec_so_far += \"\\n\" + para\n",
    "    return title_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c29d5f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_text = extract_section_title(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6b1004c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['', '1 Introduction', '2 Chain-of-Thought Prompting', '3 Arithmetic Reasoning', '4 Commonsense Reasoning', '5 Symbolic Reasoning', '6 Discussion', '7 Related Work'])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sec_text.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bf473eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "Generate a bullet style summary for the following text: {section}\"\"\"\n",
    "\n",
    "summary_prompt = PromptTemplate(\n",
    "    input_variables=[\"section\"],\n",
    "    template=template,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f07df1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.base import Chain\n",
    "\n",
    "summary_chain = LLMChain(llm=openai, prompt=summary_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "08c29bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty print constants\n",
    "\n",
    "bold = '\\033[1m'\n",
    "unbold = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "27515cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1 Introduction\u001b[0m\n",
      "\n",
      "\n",
      "• This work explores how the reasoning ability of large language models can be unlocked by a simple method motivated by two ideas: (1) generating natural language rationales that lead to the final answer and (2) in-context few-shot learning via prompting. \n",
      "• The approach, chain-of-thought prompting, consists of prompting the model with a few input–output exemplars demonstrating the task, along with a chain of thought - a series of intermediate natural language reasoning steps that lead to the final output. \n",
      "• Empirical evaluations show that chain-of-thought prompting outperforms standard prompting, sometimes to a striking degree. \n",
      "• On the GSM8K benchmark of math word problems, chain-of-thought prompting with PaLM 540B outperforms standard prompting by a large margin and achieves new state-of-the-art performance.\n",
      "• This work underscores how large language models can learn via a few examples with natural language data about the task.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\u001b[1m2 Chain-of-Thought Prompting\u001b[0m\n",
      "\n",
      "\n",
      "-Consider one's own thought process when solving a complicated reasoning task such as a multi-step math word problem\n",
      "-Goal is to endow language models with the ability to generate a similar chain of thought\n",
      "-Attractive properties of chain-of-thought prompting include being able to decompose multi-step problems, providing an interpretable window into the model's behavior, being applicable to any task humans can solve via language, and being elicited in off-the-shelf language models\n",
      "-Empirical experiments will observe the utility of chain-of-thought prompting for arithmetic, commonsense, and symbolic reasoning\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\u001b[1m3 Arithmetic Reasoning\u001b[0m\n",
      " language models.\n",
      "\n",
      "Bullet Style Summary:\n",
      "- We explore chain-of-thought prompting for various language models on multiple math word problem benchmarks.\n",
      "- 8 few-shot exemplars with chains of thought were manually composed for the evaluation split.\n",
      "- We evaluate 5 large language models: GPT-3, LaMDA, PaLM, UL2 20B, and Codex.\n",
      "- Chain-of-thought prompting when used with the 540B parameter language model performs comparably with task-specific finetuned models on several tasks, even achieving new state of the art on the challenging GSM8K benchmark.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\u001b[1m4 Commonsense Reasoning\u001b[0m\n",
      "\n",
      "\n",
      "• Chain of thought is particularly suitable for math word problems, but can be applied to a wide range of commonsense reasoning problems involving physical and human interactions.\n",
      "• Five datasets were used to evaluate chain of thought prompting: CSQA, StrategyQA, Date Understanding, Sports Understanding, and SayCan.\n",
      "• Experiments showed improvements with chain of thought prompting, with PaLM 540B outperforming the prior state of the art on StrategyQA and outperforming an unaided sports enthusiast on Sports Understanding.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\u001b[1m5 Symbolic Reasoning\u001b[0m\n",
      "\n",
      "\n",
      "• Last Letter Concatenation and Coin Flip are two symbolic reasoning tasks that require chain-of-thought prompting for language models to perform. \n",
      "• With chain-of-thought prompting, language models with 540B parameters achieved almost 100% solve rate for in-domain tasks, and length generalization for OOD tasks. \n",
      "• For OOD tasks, standard prompting failed to achieve length generalization whereas chain-of-thought prompting facilitated it.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\u001b[1m6 Discussion\u001b[0m\n",
      "\n",
      "\n",
      "- Explored chain-of-thought prompting as a simple mechanism for eliciting multi-step reasoning behavior in large language models\n",
      "- Improved performance by a large margin on arithmetic reasoning; stronger than ablations and robust to different annotators, exemplars, and language models\n",
      "- Tested common names using GPT-3 davinci with 10 names and got all but one correct\n",
      "- Experiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought reasoning makes it generally applicable\n",
      "- For symbolic reasoning, chain-of-thought prompting facilitates OOD generalization to longer sequence lengths\n",
      "- Chain-of-thought prompting expands the set of tasks that large language models can perform successfully\n",
      "- Limitations: no guarantee of correct reasoning paths, costly to serve in real-world applications, emergence of chain-of-thought reasoning only at large model scales\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\u001b[1m7 Related Work\u001b[0m\n",
      "\n",
      "\n",
      "- This work is inspired by many research areas, including the use of intermediate steps to solve reasoning problems (Ling et al., 2017; Cobbe et al., 2021) and program synthesis (Nye et al., 2021). \n",
      "- It also relates closely to the large body of recent work on prompting, such as automatically learning prompts (Lester et al., 2021) or giving models instructions describing a task (Wei et al., 2022a; Sanh et al., 2022; Ouyang et al., 2022). \n",
      "- This paper takes the orthogonal direction of augmenting the outputs of language models with a chain of thought.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, val in sec_text.items():\n",
    "    if key != \"\":\n",
    "        print(f\"{bold}{key}{unbold}\")\n",
    "        if len(val.split(\" \")) > 1000:\n",
    "            val = \" \".join(val.split(\" \")[:1000])\n",
    "        print(summary_chain.run(section=f\"{val}\"))\n",
    "        print(\"\\n\")\n",
    "        print(\"-\" * 80)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bc98f786",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Work Pending and to be done during the project\n",
    "1. Better extraction of section and content\n",
    "2. Split/merge sections with long subsections\n",
    "3. Experiment with non-gpt generative models finetuned especially for this task using data collected\n",
    "4. Prepare a semi-decent UI\n",
    "5. Add graphics\n",
    "6. Add style based PPT \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93920ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
